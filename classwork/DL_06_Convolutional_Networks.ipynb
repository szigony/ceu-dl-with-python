{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/DL.png\" width=110 align=\"left\" style=\"margin-right: 10px\">\n",
    "\n",
    "# Introduction to Deep Learning\n",
    "\n",
    "## 06. Convolutional Networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Convolutional Neural Network (CNN)](https://keras.io/layers/convolutional/)\n",
    "\n",
    "<img src=\"./pics/external/cnn/cnn.png\" width=600 alt=\"Typical cnn.png\"><br>By <a href=\"//commons.wikimedia.org/w/index.php?title=User:Aphex34&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"User:Aphex34 (page does not exist)\">Aphex34</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=45679374\">Link</a>\n",
    "\n",
    "Previously we have built a fully connected dense layer to categorize the MNIST dataset of hand-written numbers. We managed to build a pretty accurate system but we used each pixel as a separate input. Large part of the information is in the spatial structure of the image. Convolutional networks are specialized neural networks. They differ from the fully connected networks by assuming that the input contains spatial information (eg. it's a picture).  \n",
    "When we look at a picture we are \"looking at the big picture\", not individual pixels. Our eye is a specialized instrument which is built to recognize shapes, edges, etc. We don't really care how grey is a cat's fur, we know it's a cat because of the overall features, the shapes and contours. Convolutional networks are specialized similarly and build this knowledge into the architecture to execute faster forward propagations and do more accurate training.\n",
    "\n",
    "A Convolutional Neural Network consists of different building blocks:\n",
    "- Convolutional layers: feature extraction\n",
    "- Pooling layers: feature selection\n",
    "- Dense layers: classification\n",
    "\n",
    "### Building blocks\n",
    "\n",
    "#### Convolutional layer\n",
    "\n",
    "\n",
    "<img src=\"./pics/cnn/convolutional_layer.png\" width=400 align=\"left\">\n",
    "<br><br>\n",
    "\n",
    "Instead of vertical line of input neurons, we arrange input neurons into 2d mesh, as they were in the images. We use these input neurons but instead of connecting every input into a neuron in the hidden layer, we only connect them to a small local region. It's more appropriate to look at this from the other way around: from one neuron in the hidden layer, we connect to a region in the input image that we call the hidden neuron's *local receptive field*. \n",
    "\n",
    "The hidden neurons have weights and a bias as well just like a regular hidden neuron in a dense network, and their purpose is to learn the related specific region of the input image. In practice, they act as a learnable filter, so we can look at them as a filtering layer which purpose is to detect features in the input. In case of images these features could be edges, or even shapes. \n",
    "\n",
    "<div style=\"align: clear\"/>    \n",
    "<br><br><br>\n",
    "\n",
    "Their name comes from this view:  \n",
    "> _In mathematics convolution is a mathematical operation on two functions (f and g) to produce a third function that expresses how the shape of one is modified by the other._ - [source](https://en.wikipedia.org/wiki/Convolution)\n",
    "    \n",
    "    \n",
    "<img src=\"./pics/external/cnn/convolution.gif\" alt=\"Convolution of box signal with itself2.gif\"><br>By <a href=\"./pics/external/cnn/convolution.gif\" title=\"File:Convolution of box signal with itself.gif\">Convolution_of_box_signal_with_itself.gif</a>: Brian Amberg\n",
    "derivative work: <a href=\"//commons.wikimedia.org/wiki/User:Tinos\" title=\"User:Tinos\">Tinos</a> (<a href=\"//commons.wikimedia.org/wiki/User_talk:Tinos\" title=\"User talk:Tinos\"><span class=\"signature-talk\">talk</span></a>) - <a href=\"//commons.wikimedia.org/wiki/File:Convolution_of_box_signal_with_itself.gif\" title=\"File:Convolution of box signal with itself.gif\">Convolution_of_box_signal_with_itself.gif</a>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=11003835\">Link</a>\n",
    "\n",
    "This convolution operation is executed by each hidden layer neuron, just like a **rolling window** over the input image. Since each hidden neuron is responsible for a region of input neurons, we can deduct that the output of this layer will be smaller by a pixel in each dimension - in fact that's one parameter of the process. Let's consider convolution in the animation below: a green filtering operation is sliding through the blue input values resulting the red output features.\n",
    "\n",
    "<div style=\"display: inline-block;\">\n",
    "    <img src=\"./pics/external/cnn/sliding_window.gif\" width=400 align='left'>\n",
    "    <img src=\"./pics/external/cnn/filter.png\" width=400 align='left'>\n",
    "</div>\n",
    "\n",
    "<div style=\"align: clear\"/>\n",
    "<br>\n",
    "Animation by <a href=\"https://towardsdatascience.com/@ardendertat\">Arden Dertat</a>, <a href=\"https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2\">Link</a> \n",
    "Image by <a href=\"https://towardsdatascience.com/@ardendertat\">Arden Dertat</a>, <a href=\"https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2\">Link</a>\n",
    "</div>\n",
    "\n",
    "It is important to note that **the weights** in a layer **are shared** among the neurons on a given layer. We can express the output of a hidden neuron with a 5 x 5 size receptive field as:\n",
    "$$\\sigma\\left(b + \\sum_{l=0}^4 \\sum_{m=0}^4  w_{l,m} a_{j+l, k+m} \\right)$$\n",
    "where $\\sigma$ is the activation function, $w_{l,m}$ is the shared weight, and finally $a_{x, y}$ is the input neuron at the $x, y$. The generated output from the input using a convolutional hidden layer is called *feature map* and the above equition is defining a *kernel* or *filter*.  \n",
    "Multiple convolutional layer can be used on the same input, each responsible for a different feature mapping. Weights and biases only shared inside a layer, but not between these feature maps.\n",
    "\n",
    "The three main parameter to watch out when defining a convolutional layer is:\n",
    "\n",
    "- __depth__: The number of filters we'd like to use\n",
    "- __stride__: The size of the step the convolution filter moves each time\n",
    "- __padding__: the size of the zero-padding around the input\n",
    "\n",
    "#### Pooling layer\n",
    "\n",
    "<img src=\"./pics/external/cnn/pooling.png\" width=400><br>By <a href=\"https://cs.stanford.edu/people/karpathy/\">Andrej Karpathy</a>, <a href=\"http://cs231n.github.io/convolutional-networks/\">Link</a>\n",
    "\n",
    "Pooling is much more straightforward: It reduces the dimensionality of the input by downsampling it. It usually follows a convolutional layer, and is defined by a *window size* and an *aggregation function* to create an approximate output of the input. We can look at them as a way to drop the location information, since the found  Using a poolig layer prevents overfitting, reduces the number of weights in the consecutive layers, shortens training time, and also keeps the important informations. The most common aggregation function is **max**, in this case we call the layer a *max-pooling* layer. Other common choice is the *L2-pooling* which takes the square root of the sum of the squeares inside the pool.\n",
    "\n",
    "The two important parameters are:\n",
    "- **pool size**: the size of the window to aggregate\n",
    "- **stride**: the stepsize\n",
    "\n",
    "#### Other layers\n",
    "\n",
    "- **Fully-connected layer**: Standard Dense fully connected layer with proper a cost function.\n",
    "- **Flatten**: Transforming layer. Multi-dimensional neurons are flattened into a single layer of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Practice\n",
    "\n",
    "### MNIST\n",
    "\n",
    "Let's improve our previous naive solution by building a CNN classifier for the hand digits dataset.  \n",
    "The previously used dataset was a simplyfied version, the image resolutions were *8 x 8*, and the dataset contained only ~2000 items. This dataset however contains 70,000 images (60k training and 10k test image) of *28 x 28* resolution.\n",
    "\n",
    "Let's look into the data first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24ea0e133c8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "sns.heatmap(X_train[2], cmap='gray', cbar=False, square=True, \n",
    "            xticklabels=False, yticklabels=False, annot=False, \n",
    "            ax=ax1)\n",
    "sns.heatmap(X_train[5], cmap='gray', cbar=False, square=True, \n",
    "            xticklabels=False, yticklabels=False, annot=False, \n",
    "            ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D convolutional layers expects images with multiple channels, eg. RGB or YMCK color channels, or multiple feature maps in deeper levels of the network. We examine the shape of our dataset first to reassure the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1), (60000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dimensions of the data is *records x height x width*, which is missing one dimension: the channel information.  \n",
    "Let's reshape it to the desired shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(*X_train.shape, 1)\n",
    "X_test = X_test.reshape(*X_test.shape, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is in the desired format, let's design our network.  \n",
    "For the first iteration, our network will be the following:\n",
    "- convolution layer:\n",
    "    - 3 x 3 receptive field\n",
    "    - 1 steps at a time\n",
    "    - 32 filters\n",
    "    - `relu` activation \n",
    "- maxpooling layer:\n",
    "    - 2 x 2 pool layer\n",
    "    - non-ovelapping slide\n",
    "- flattening\n",
    "- dense layer with `softmax` activation\n",
    "\n",
    "**Questions:** \n",
    "- What is the shape of the inputs?\n",
    "- How many output neuron do we need?\n",
    "- Which loss function should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)  # what is the input shape?\n",
    "output_size = 10  # how many output neurons do we need?\n",
    "loss_function = 'sparse_categorical_crossentropy'  # which loss function should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(output_size))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- How many parameters are in the convolution layer? Why?\n",
    "- What will be the output shape of the pooling layer?\n",
    "- How many parameters are in the dense network?\n",
    "\n",
    "Check the answers by uncommenting the code in the cell below and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                54090     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 54,410\n",
      "Trainable params: 54,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras callbacks\n",
    "\n",
    "Training deep networks could easily take multiple hours, days, in some extreme cases, weeks. During that period many thing could go wrong, many interesting thing can happen with the network.  \n",
    "After starting the training process the only interaction we have with the process is to observe the results or stop the process. However there is a backdoor available: the callbacks.  \n",
    "Callbacks are functions which is called at the end of each epoch. You can create your [own](https://www.tensorflow.org/guide/keras/custom_callback) or use the [built-in solutions](https://keras.io/callbacks/).\n",
    "\n",
    "##### Early stopping\n",
    "In this case we don't want perfect results but we want them quick. We are going to use the `EarlyStopping` callback which simply does what its name suggests: stops early if the training process does not improve. It has a `patience` parameter which allows *n* epochs without improvement before stopping the process.  \n",
    "To help it's work, we are going to provide **validation data** to the fitting process. There are two ways to do this: manually create the validation set or by setting a split ratio. We're going to do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "54000/54000 [==============================] - 14s 255us/step - loss: 0.6259 - accuracy: 0.9397 - val_loss: 0.1104 - val_accuracy: 0.9693\n",
      "Epoch 2/100\n",
      "54000/54000 [==============================] - 14s 252us/step - loss: 0.1013 - accuracy: 0.9698 - val_loss: 0.1293 - val_accuracy: 0.9698\n",
      "Epoch 3/100\n",
      "54000/54000 [==============================] - 13s 240us/step - loss: 0.0808 - accuracy: 0.9757 - val_loss: 0.1126 - val_accuracy: 0.9710\n",
      "Epoch 4/100\n",
      "54000/54000 [==============================] - 13s 238us/step - loss: 0.0717 - accuracy: 0.9792 - val_loss: 0.1184 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24ea0efbcc8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          batch_size=16,\n",
    "          epochs=100,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 72us/step\n",
      "test loss: 0.11674221257695971, test acc: 0.973800003528595\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'test loss: {loss}, test acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### MNIST dataset and LeNet-5\n",
    "\n",
    "Build the famous LeNet-5 convolutional network based on the published [paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf).  \n",
    "Note that in the paper, the images in the MNIST dataset were in *32 x 32* resolution - let's ignore this difference, and tweak the model accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build the model\n",
    "lenet5 = Sequential()\n",
    "\n",
    "lenet5.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), input_shape=input_shape))\n",
    "lenet5.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "lenet5.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1)))\n",
    "lenet5.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "lenet5.add(Flatten())\n",
    "lenet5.add(Dense(84, activation='sigmoid'))\n",
    "lenet5.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 2. Compile the model\n",
    "lenet5.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 6)         156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 16)          2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                21588     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 25,010\n",
      "Trainable params: 25,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lenet5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "54000/54000 [==============================] - 18s 335us/step - loss: 0.8572 - accuracy: 0.7244 - val_loss: 0.3350 - val_accuracy: 0.9025\n",
      "Epoch 2/100\n",
      "54000/54000 [==============================] - 18s 330us/step - loss: 0.3771 - accuracy: 0.8845 - val_loss: 0.3220 - val_accuracy: 0.9020\n",
      "Epoch 3/100\n",
      "54000/54000 [==============================] - 17s 320us/step - loss: 0.3316 - accuracy: 0.8971 - val_loss: 0.2255 - val_accuracy: 0.9355\n",
      "Epoch 4/100\n",
      "54000/54000 [==============================] - 17s 316us/step - loss: 0.3049 - accuracy: 0.9059 - val_loss: 0.2430 - val_accuracy: 0.9263\n",
      "Epoch 5/100\n",
      "54000/54000 [==============================] - 17s 316us/step - loss: 0.3006 - accuracy: 0.9063 - val_loss: 0.2699 - val_accuracy: 0.9125\n",
      "Epoch 6/100\n",
      "54000/54000 [==============================] - 17s 318us/step - loss: 0.3040 - accuracy: 0.9062 - val_loss: 0.2265 - val_accuracy: 0.9320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24ea359f7c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Fit the model\n",
    "lenet5.fit(X_train, y_train, batch_size=16, epochs=100, validation_split=0.1, callbacks=[EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 88us/step\n",
      "test loss: 0.26241378702521323, test acc: 0.9210000038146973\n"
     ]
    }
   ],
   "source": [
    "loss, acc = lenet5.evaluate(X_test, y_test)\n",
    "print(f'test loss: {loss}, test acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good job!\n",
    "\n",
    "In the next chapter we'll talk about how can we create vectorial representation of complex concepts using a neural network to embed the data in [DL 07 Embeddings](./DL_07_Embeddings.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
