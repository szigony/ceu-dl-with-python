{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/DL.png\" width=110 align=\"left\" style=\"margin-right: 10px\">\n",
    "\n",
    "# Introduction to Deep Learning\n",
    "\n",
    "## 09. Reinforcement Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Reinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences.\n",
    "\n",
    "Though both supervised and reinforcement learning use mapping between input and output, unlike supervised learning where feedback provided to the agent is correct set of actions for performing a task, reinforcement learning uses rewards and punishment as signals for positive and negative behavior.\n",
    "\n",
    "As compared to unsupervised learning, reinforcement learning is different in terms of goals. While the goal in unsupervised learning is to find similarities and differences between data points, in reinforcement learning the goal is to find a suitable action model that would maximize the total cumulative reward of the agent. The figure below represents the basic idea and elements involved in a reinforcement learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work?\n",
    "\n",
    "<img src=\"./pics/external/rl/environment.jpg\" alt=\"Reinforcement learning Environment - Agent\">\n",
    "<br>Image from <a href=\"https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html\">5 Things You Need to Know about Reinforcement Learning</a>, by <a href=\"https://www.kdnuggets.com/author/shweta-bhatt\">Shweta Bhatt</a>, Youplus.\n",
    "\n",
    "Main components of the Reinforcement Learning environment\n",
    "- **Agent**: The controlled entity in the environment which can interact with the environment, execute actions and observe the rewards\n",
    "- **Environment**: The abstract world in which the agent operates\n",
    "- **State**: $S_t$ is the current state of the agent in the environment\n",
    "- **Action**: $A_t$ is the interaction the agent selected to do in the current environment\n",
    "- **Reward**: $R_t$ is the reward the agent receives from the environment as a response for its action. The goal of the agent is to maximize the received reward.\n",
    "- **Policy**: $\\pi \\left(A_t = a | S_t = s, \\theta \\right)$ is the decision strategy of the agent. Based on the current state and the learned parameters the agent selects an action from its repertoire.\n",
    "- **Value**: $V_{\\pi}\\left(S\\right)$ The expected sum of rewards if the agent follows $\\pi$ policy.\n",
    "\n",
    "<img src=\"./pics/external/rl/mari-o.gif\" alt=\"MarI/O\">\n",
    "<br>Image from <a href=\"https://www.youtube.com/watch?v=qv6UVOQ0F44\">MarI/O - Machine Learning for Video Games</a>, by <a href=\"https://www.kdnuggets.com/author/shweta-bhatt\">Seth Bling</a>.\n",
    "\n",
    "The easiest way to get a grasp at Reinforcement Learning is through games: Using Super Mario World as an example, the agent's (aka. Mario's) goal is to get to the end of the level. The **world** around him **is the environment** where he must jump trough the enemies, (optionally) collect coins and kill enemies by jumping on them. Mario get's **rewards for progressing through the level** with a huge bonus when reaching the end but also got punishment (aka **negative reward**) **for walking into enemies**, or **falling into chasms** or letting the time run out. The **states are the position** of Mario **and the actual visible layout**; the **actions are the buttonpresses** on the controller.\n",
    "\n",
    "At each timestamp the agent evaluates the possible actions considering the current state and based on the policy, it chooses the most beneficial action. Leraning an optimal policy requires the agent to **explore** the action space while **exploit** the incoming rewards - maximize the overall reward. This dual problem is called **exploration vs exploitation** tradoff.\n",
    "\n",
    "The matematical frameworks to handle such problems are the **Markov Decision Process**es (MDP). With the help of MDPs, we can describe the environment in RL problems and formalize the process. MDPs describe the environment as a finite set of $S$ states, $A$ possible actions, $R$ reward values, and transitive possibilities $P(s', s | a)$. Real world environments does not contain prior knowledge about the environment - to handle this problem, RL has a set of processes called model-free approches.\n",
    "\n",
    "Since Reinforcement Learning is a whole separate branch of the machine learning algorithms, we won't go deeper into the field, instead let's see how can we realize a specific model free learning method called **Q-learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Two commonly used model-free algorithms are SARSA (State-Action-Reward-State-Action) and Q-learning. The main difference is how they explore their environment. SARSA is an on-policy method which learnd the overall value of an action by using the current policy to project its effectiveness. Q-learning is an off-policy algorithm, it uses an another (otherwise unused) policy to determine the value of an action. These methods are fairly simple but are not able to estimate values for unseen states.\n",
    "\n",
    "To overcome this problem, we'll use the Deep Q-Networks algorithm to estimate Q-values. This algorithm is able to handle low-dimensional discrete action spaces. To handle continuous problems, Deep Deterministic Policy Gradients are a good starting point, however it is out of the scope of this discussion. \n",
    "\n",
    "Deep Q networks revolves around updating Q-values to estimate an action's value in a given state. The value update rule is the core of the Q-learning algorithm and can be written as:\n",
    "\n",
    "$$\\begin{align}\n",
    "    Q^{\\textrm{new}}\\left( s_t, a_t \\right) & =\n",
    "    \\underbrace{Q \\left( s_t, a_t \\right)}_\\textrm{old value} + \n",
    "    \\underbrace{\\alpha}_\\textrm{learning rate} \\cdot\n",
    "    \\overbrace{\\Bigg( \n",
    "        \\underbrace{\n",
    "            \\underbrace{r_t}_\\textrm{reward} +\n",
    "            \\underbrace{\\gamma}_\\textrm{discount factor} \\cdot\n",
    "            \\underbrace{\\max_{a}{Q\\left( s_{t+1}, a \\right)}}_\\textrm{estimate of optimal future value}\n",
    "        }_\\textrm{new value (temporali difference target)} -\n",
    "        \\underbrace{Q\\left( s_t, a_t \\right)}_\\textrm{old value}\n",
    "    \\Bigg)}^\\textrm{temporal difference}\n",
    "\\end{align}$$\n",
    "\n",
    "DQNs will learn online, meaning that we donâ€™t simply create a bunch of trial/training data and feed it into the model, but we create training data through the trials we run and feed this information into it directly after running the trial instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Practice\n",
    "\n",
    "DQNs are not part of any widespread deep learning library, so we'll have to implement the algoritm ourselves. To make it easier to understand the process let's consider a small (but not at all straightforward!) problem:\n",
    "\n",
    "#### Building a hill-climber car\n",
    "\n",
    "Since Reinforcement Learning living its renaissance there are frameworks to easily simulate the RL environment. One of the most popular framework is called gym. It contains the problem [Mountain Car](https://gym.openai.com/envs/MountainCar-v0/) where the goal is to drive up a car to a hill. The car in the problem does not have enough power to go up the hill, so the agent has to rely on building up a momentum. \n",
    "\n",
    "The state of the agent is the elevation of the car and the velocity; the available actions are accelerate to the left (0), don't accelerate (1), and accelerate to the right (2). The simulation ends if the car's elevation is above 0.5, or more than 200 episodes has passed.\n",
    "\n",
    "First, let's see the problem using random actions in each episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self, problem):\n",
    "        self.env = gym.make(problem)\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.env.reset()\n",
    "        return self.env\n",
    "    \n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Random solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "action: 1\n",
      "observation: [-0.43451655 -0.000665  ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 1:\n",
      "action: 1\n",
      "observation: [-0.43584174 -0.00132519]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 2:\n",
      "action: 1\n",
      "observation: [-0.43781754 -0.00197579]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 3:\n",
      "action: 0\n",
      "observation: [-0.44142961 -0.00361208]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 4:\n",
      "action: 0\n",
      "observation: [-0.44665174 -0.00522212]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 5:\n",
      "action: 2\n",
      "observation: [-0.45144585 -0.00479411]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 6:\n",
      "action: 0\n",
      "observation: [-0.45777689 -0.00633104]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 7:\n",
      "action: 2\n",
      "observation: [-0.4635984  -0.00582151]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 8:\n",
      "action: 1\n",
      "observation: [-0.46986748 -0.00626908]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 9:\n",
      "action: 2\n",
      "observation: [-0.47553781 -0.00567032]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 10:\n",
      "action: 0\n",
      "observation: [-0.48256734 -0.00702953]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 11:\n",
      "action: 0\n",
      "observation: [-0.49090384 -0.00833649]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 12:\n",
      "action: 2\n",
      "observation: [-0.49848515 -0.00758131]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 13:\n",
      "action: 2\n",
      "observation: [-0.50525463 -0.00676949]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 14:\n",
      "action: 2\n",
      "observation: [-0.51116163 -0.005907  ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 15:\n",
      "action: 1\n",
      "observation: [-0.51716189 -0.00600026]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 16:\n",
      "action: 0\n",
      "observation: [-0.52421042 -0.00704853]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 17:\n",
      "action: 1\n",
      "observation: [-0.53125436 -0.00704394]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 18:\n",
      "action: 1\n",
      "observation: [-0.53824089 -0.00698653]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 19:\n",
      "action: 1\n",
      "observation: [-0.54511764 -0.00687675]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 20:\n",
      "action: 1\n",
      "observation: [-0.55183311 -0.00671547]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 21:\n",
      "action: 1\n",
      "observation: [-0.55833707 -0.00650397]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 22:\n",
      "action: 1\n",
      "observation: [-0.56458097 -0.0062439 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 23:\n",
      "action: 2\n",
      "observation: [-0.56951828 -0.00493731]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 24:\n",
      "action: 0\n",
      "observation: [-0.57511228 -0.005594  ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 25:\n",
      "action: 1\n",
      "observation: [-0.58032146 -0.00520918]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 26:\n",
      "action: 2\n",
      "observation: [-0.58410728 -0.00378581]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 27:\n",
      "action: 2\n",
      "observation: [-0.58644177 -0.00233449]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 28:\n",
      "action: 1\n",
      "observation: [-0.58830772 -0.00186595]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 29:\n",
      "action: 0\n",
      "observation: [-0.5906914  -0.00238368]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 30:\n",
      "action: 0\n",
      "observation: [-0.59357527 -0.00288387]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 31:\n",
      "action: 1\n",
      "observation: [-0.59593817 -0.0023629 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 32:\n",
      "action: 2\n",
      "observation: [-0.59676277 -0.0008246 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 33:\n",
      "action: 0\n",
      "observation: [-0.59804303 -0.00128027]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 34:\n",
      "action: 0\n",
      "observation: [-0.5997696  -0.00172656]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 35:\n",
      "action: 0\n",
      "observation: [-0.60192984 -0.00216024]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 36:\n",
      "action: 1\n",
      "observation: [-0.60350799 -0.00157815]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 37:\n",
      "action: 0\n",
      "observation: [-0.60549254 -0.00198456]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 38:\n",
      "action: 0\n",
      "observation: [-0.60786906 -0.00237651]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 39:\n",
      "action: 0\n",
      "observation: [-0.61062025 -0.0027512 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 40:\n",
      "action: 0\n",
      "observation: [-0.61372618 -0.00310592]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 41:\n",
      "action: 0\n",
      "observation: [-0.61716435 -0.00343817]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 42:\n",
      "action: 1\n",
      "observation: [-0.61990996 -0.00274561]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 43:\n",
      "action: 1\n",
      "observation: [-0.62194325 -0.00203329]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 44:\n",
      "action: 1\n",
      "observation: [-0.6232496  -0.00130636]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 45:\n",
      "action: 2\n",
      "observation: [-6.22819661e-01  4.29942136e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 46:\n",
      "action: 0\n",
      "observation: [-6.22656502e-01  1.63158263e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 47:\n",
      "action: 1\n",
      "observation: [-0.6217613  0.0008952]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 48:\n",
      "action: 2\n",
      "observation: [-0.61914047  0.00262083]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 49:\n",
      "action: 1\n",
      "observation: [-0.61581285  0.00332762]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 50:\n",
      "action: 2\n",
      "observation: [-0.61080241  0.00501044]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 51:\n",
      "action: 0\n",
      "observation: [-0.60614538  0.00465703]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 52:\n",
      "action: 0\n",
      "observation: [-0.60187556  0.00426982]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 53:\n",
      "action: 2\n",
      "observation: [-0.59602405  0.00585152]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 54:\n",
      "action: 1\n",
      "observation: [-0.5896336   0.00639044]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 55:\n",
      "action: 1\n",
      "observation: [-0.58275113  0.00688247]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 56:\n",
      "action: 2\n",
      "observation: [-0.57442735  0.00832379]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 57:\n",
      "action: 0\n",
      "observation: [-0.56672382  0.00770353]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 58:\n",
      "action: 1\n",
      "observation: [-0.55869776  0.00802606]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 59:\n",
      "action: 1\n",
      "observation: [-0.55040894  0.00828882]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 60:\n",
      "action: 1\n",
      "observation: [-0.54191926  0.00848968]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 61:\n",
      "action: 0\n",
      "observation: [-0.53429225  0.00762701]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 62:\n",
      "action: 0\n",
      "observation: [-0.52758505  0.0067072 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 63:\n",
      "action: 0\n",
      "observation: [-0.52184795  0.0057371 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 64:\n",
      "action: 1\n",
      "observation: [-0.51612398  0.00572397]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 65:\n",
      "action: 1\n",
      "observation: [-0.51045608  0.00566791]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 66:\n",
      "action: 1\n",
      "observation: [-0.50488671  0.00556936]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 67:\n",
      "action: 0\n",
      "observation: [-0.50045761  0.0044291 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 68:\n",
      "action: 1\n",
      "observation: [-0.49620193  0.00425568]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 69:\n",
      "action: 1\n",
      "observation: [-0.4921515   0.00405043]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 70:\n",
      "action: 1\n",
      "observation: [-0.48833657  0.00381493]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 71:\n",
      "action: 2\n",
      "observation: [-0.48378562  0.00455095]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 72:\n",
      "action: 2\n",
      "observation: [-0.47853255  0.00525307]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 73:\n",
      "action: 0\n",
      "observation: [-0.47461645  0.0039161 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 74:\n",
      "action: 2\n",
      "observation: [-0.4700664   0.00455005]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 75:\n",
      "action: 2\n",
      "observation: [-0.46491612  0.00515028]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 76:\n",
      "action: 1\n",
      "observation: [-0.46020369  0.00471243]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 77:\n",
      "action: 0\n",
      "observation: [-0.45696386  0.00323983]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 78:\n",
      "action: 2\n",
      "observation: [-0.45322047  0.00374339]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 79:\n",
      "action: 1\n",
      "observation: [-0.45000101  0.00321947]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 80:\n",
      "action: 1\n",
      "observation: [-0.44732905  0.00267196]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 81:\n",
      "action: 0\n",
      "observation: [-0.44622414  0.00110491]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 82:\n",
      "action: 2\n",
      "observation: [-0.44469434  0.0015298 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 83:\n",
      "action: 0\n",
      "observation: [-4.44750812e-01 -5.64732350e-05]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 84:\n",
      "action: 0\n",
      "observation: [-0.44639315 -0.00164233]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 85:\n",
      "action: 2\n",
      "observation: [-0.44760936 -0.00121621]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 86:\n",
      "action: 1\n",
      "observation: [-0.44939057 -0.00178121]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 87:\n",
      "action: 2\n",
      "observation: [-0.45072375 -0.00133319]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 88:\n",
      "action: 1\n",
      "observation: [-0.45259916 -0.0018754 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 89:\n",
      "action: 1\n",
      "observation: [-0.45500304 -0.00240388]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 90:\n",
      "action: 1\n",
      "observation: [-0.45791777 -0.00291473]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 91:\n",
      "action: 2\n",
      "observation: [-0.46032192 -0.00240415]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 92:\n",
      "action: 2\n",
      "observation: [-0.46219781 -0.00187589]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 93:\n",
      "action: 1\n",
      "observation: [-0.4645316  -0.00233379]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 94:\n",
      "action: 1\n",
      "observation: [-0.46730609 -0.00277448]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 95:\n",
      "action: 2\n",
      "observation: [-0.46950076 -0.00219467]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 96:\n",
      "action: 1\n",
      "observation: [-0.47209939 -0.00259863]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 97:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 2\n",
      "observation: [-0.47408273 -0.00198334]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 98:\n",
      "action: 2\n",
      "observation: [-0.47543608 -0.00135335]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 99:\n",
      "action: 2\n",
      "observation: [-0.47614939 -0.00071331]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 100:\n",
      "action: 0\n",
      "observation: [-0.47821737 -0.00206798]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 101:\n",
      "action: 2\n",
      "observation: [-0.47962466 -0.00140729]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 102:\n",
      "action: 2\n",
      "observation: [-0.4803608  -0.00073614]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 103:\n",
      "action: 1\n",
      "observation: [-0.48142032 -0.00105952]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 104:\n",
      "action: 0\n",
      "observation: [-0.48379533 -0.00237501]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 105:\n",
      "action: 0\n",
      "observation: [-0.48746816 -0.00367283]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 106:\n",
      "action: 0\n",
      "observation: [-0.49241144 -0.00494328]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 107:\n",
      "action: 0\n",
      "observation: [-0.49858828 -0.00617684]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 108:\n",
      "action: 2\n",
      "observation: [-0.50395253 -0.00536425]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 109:\n",
      "action: 1\n",
      "observation: [-0.50946403 -0.00551151]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 110:\n",
      "action: 2\n",
      "observation: [-0.51408152 -0.00461749]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 111:\n",
      "action: 1\n",
      "observation: [-0.51877037 -0.00468886]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 112:\n",
      "action: 0\n",
      "observation: [-0.52449544 -0.00572507]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 113:\n",
      "action: 2\n",
      "observation: [-0.52921378 -0.00471834]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 114:\n",
      "action: 1\n",
      "observation: [-0.53389001 -0.00467623]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 115:\n",
      "action: 2\n",
      "observation: [-0.53748907 -0.00359906]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 116:\n",
      "action: 1\n",
      "observation: [-0.54098399 -0.00349491]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 117:\n",
      "action: 2\n",
      "observation: [-0.54334857 -0.00236458]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 118:\n",
      "action: 1\n",
      "observation: [-0.54556511 -0.00221655]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 119:\n",
      "action: 0\n",
      "observation: [-0.54861703 -0.00305192]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 120:\n",
      "action: 2\n",
      "observation: [-0.55048149 -0.00186446]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 121:\n",
      "action: 1\n",
      "observation: [-0.55214454 -0.00166305]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 122:\n",
      "action: 1\n",
      "observation: [-0.55359377 -0.00144922]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 123:\n",
      "action: 0\n",
      "observation: [-0.55581833 -0.00222456]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 124:\n",
      "action: 2\n",
      "observation: [-0.55680162 -0.00098329]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 125:\n",
      "action: 2\n",
      "observation: [-5.56536308e-01  2.65316256e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 126:\n",
      "action: 1\n",
      "observation: [-5.56024362e-01  5.11945942e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 127:\n",
      "action: 0\n",
      "observation: [-5.56269607e-01 -2.45245525e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 128:\n",
      "action: 2\n",
      "observation: [-0.55527021  0.00099939]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 129:\n",
      "action: 2\n",
      "observation: [-0.55303364  0.00223657]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 130:\n",
      "action: 0\n",
      "observation: [-0.55157659  0.00145705]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 131:\n",
      "action: 2\n",
      "observation: [-0.54890996  0.00266663]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 132:\n",
      "action: 0\n",
      "observation: [-0.54705367  0.00185629]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 133:\n",
      "action: 1\n",
      "observation: [-0.54502162  0.00203205]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 134:\n",
      "action: 0\n",
      "observation: [-0.54382901  0.00119261]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 135:\n",
      "action: 2\n",
      "observation: [-0.54148476  0.00234425]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 136:\n",
      "action: 2\n",
      "observation: [-0.53800644  0.00347833]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 137:\n",
      "action: 1\n",
      "observation: [-0.53442008  0.00358635]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 138:\n",
      "action: 1\n",
      "observation: [-0.53075259  0.0036675 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 139:\n",
      "action: 2\n",
      "observation: [-0.52603144  0.00472115]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 140:\n",
      "action: 0\n",
      "observation: [-0.52229205  0.00373939]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 141:\n",
      "action: 1\n",
      "observation: [-0.51856246  0.00372959]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 142:\n",
      "action: 1\n",
      "observation: [-0.51487064  0.00369182]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 143:\n",
      "action: 1\n",
      "observation: [-0.51124427  0.00362637]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 144:\n",
      "action: 2\n",
      "observation: [-0.50671055  0.00453373]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 145:\n",
      "action: 2\n",
      "observation: [-0.50130343  0.00540712]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 146:\n",
      "action: 2\n",
      "observation: [-0.49506339  0.00624003]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 147:\n",
      "action: 0\n",
      "observation: [-0.49003712  0.00502628]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 148:\n",
      "action: 1\n",
      "observation: [-0.48526213  0.00477499]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 149:\n",
      "action: 1\n",
      "observation: [-0.48077403  0.0044881 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 150:\n",
      "action: 0\n",
      "observation: [-0.47760624  0.00316779]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 151:\n",
      "action: 2\n",
      "observation: [-0.47378229  0.00382394]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 152:\n",
      "action: 0\n",
      "observation: [-0.47133058  0.00245171]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 153:\n",
      "action: 1\n",
      "observation: [-0.46926928  0.0020613 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 154:\n",
      "action: 1\n",
      "observation: [-0.46761365  0.00165563]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 155:\n",
      "action: 2\n",
      "observation: [-0.46537593  0.00223772]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 156:\n",
      "action: 1\n",
      "observation: [-0.46357267  0.00180326]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 157:\n",
      "action: 0\n",
      "observation: [-4.63217172e-01  3.55495421e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 158:\n",
      "action: 2\n",
      "observation: [-0.46231207  0.00090511]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 159:\n",
      "action: 0\n",
      "observation: [-0.46286402 -0.00055196]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 160:\n",
      "action: 1\n",
      "observation: [-0.46386898 -0.00100495]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 161:\n",
      "action: 2\n",
      "observation: [-4.64319512e-01 -4.50533438e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 162:\n",
      "action: 0\n",
      "observation: [-0.4662123  -0.00189279]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 163:\n",
      "action: 0\n",
      "observation: [-0.46953336 -0.00332106]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 164:\n",
      "action: 0\n",
      "observation: [-0.47425814 -0.00472478]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 165:\n",
      "action: 0\n",
      "observation: [-0.48035163 -0.00609348]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 166:\n",
      "action: 1\n",
      "observation: [-0.48676855 -0.00641693]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 167:\n",
      "action: 2\n",
      "observation: [-0.49246115 -0.00569259]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 168:\n",
      "action: 1\n",
      "observation: [-0.49838693 -0.00592579]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 169:\n",
      "action: 0\n",
      "observation: [-0.50550163 -0.00711469]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 170:\n",
      "action: 2\n",
      "observation: [-0.51175198 -0.00625036]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 171:\n",
      "action: 0\n",
      "observation: [-0.51909117 -0.00733919]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 172:\n",
      "action: 1\n",
      "observation: [-0.52646417 -0.00737299]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 173:\n",
      "action: 1\n",
      "observation: [-0.53381567 -0.0073515 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 174:\n",
      "action: 0\n",
      "observation: [-0.54209056 -0.00827489]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 175:\n",
      "action: 2\n",
      "observation: [-0.54922683 -0.00713627]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 176:\n",
      "action: 0\n",
      "observation: [-0.55717108 -0.00794425]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 177:\n",
      "action: 2\n",
      "observation: [-0.56386397 -0.00669288]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 178:\n",
      "action: 1\n",
      "observation: [-0.5702556  -0.00639163]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 179:\n",
      "action: 2\n",
      "observation: [-0.57529844 -0.00504284]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 180:\n",
      "action: 2\n",
      "observation: [-0.57895509 -0.00365665]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 181:\n",
      "action: 0\n",
      "observation: [-0.58319848 -0.00424338]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 182:\n",
      "action: 0\n",
      "observation: [-0.58799724 -0.00479876]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 183:\n",
      "action: 2\n",
      "observation: [-0.59131601 -0.00331877]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 184:\n",
      "action: 2\n",
      "observation: [-0.59313039 -0.00181438]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 185:\n",
      "action: 1\n",
      "observation: [-0.59442706 -0.00129667]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 186:\n",
      "action: 1\n",
      "observation: [-0.5951965  -0.00076944]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 187:\n",
      "action: 2\n",
      "observation: [-0.59443308  0.00076342]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 188:\n",
      "action: 2\n",
      "observation: [-0.5921424   0.00229069]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 189:\n",
      "action: 2\n",
      "observation: [-0.58834125  0.00380115]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 190:\n",
      "action: 2\n",
      "observation: [-0.58305758  0.00528367]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 191:\n",
      "action: 2\n",
      "observation: [-0.57633032  0.00672725]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 192:\n",
      "action: 1\n",
      "observation: [-0.56920923  0.00712109]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 193:\n",
      "action: 0\n",
      "observation: [-0.56274713  0.0064621 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 194:\n",
      "action: 0\n",
      "observation: [-0.55699209  0.00575504]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 195:\n",
      "action: 2\n",
      "observation: [-0.54998702  0.00700507]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 196:\n",
      "action: 1\n",
      "observation: [-0.54278424  0.00720278]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 197:\n",
      "action: 0\n",
      "observation: [-0.53643765  0.00634659]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 198:\n",
      "action: 0\n",
      "observation: [-0.5309948   0.00544286]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 199:\n",
      "action: 0\n",
      "observation: [-0.52649648  0.00449832]\n",
      "reward: -1.0\n",
      "done: True\n",
      "info: {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "with Environment('MountainCar-v0') as env:\n",
    "    for episode in range(1000):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        print(f\"Step {episode}:\")\n",
    "        print(f\"action: {action}\")\n",
    "        print(f\"observation: {observation}\")\n",
    "        print(f\"reward: {reward}\")\n",
    "        print(f\"done: {done}\")\n",
    "        print(f\"info: {info}\")\n",
    "\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Naive solution\n",
    "\n",
    "Build training data from random experiments, and train a NN on successful experiment samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data_preparation(env, intial_games, episodes):\n",
    "    env.reset()\n",
    "    \n",
    "    training_data = []\n",
    "    accepted_scores = []\n",
    "    for episode in range(intial_games):\n",
    "        score = 0\n",
    "        memory = []\n",
    "        previous_state = []\n",
    "        for episode in range(episodes):\n",
    "            action = random.randrange(0, 3)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if len(previous_state) > 0:\n",
    "                memory.append([previous_state, action])\n",
    "                \n",
    "            previous_state = state\n",
    "            if state[0] > -0.2:\n",
    "                reward = 1\n",
    "            \n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        if score >= score_requirement:\n",
    "            accepted_scores.append(score)\n",
    "            for data in memory:\n",
    "                state_memory, action_memory = data\n",
    "                encoded_action = [0, 0, 0]\n",
    "                encoded_action[action_memory] = 1\n",
    "                training_data.append([state_memory, encoded_action])\n",
    "        \n",
    "        env.reset()\n",
    "    \n",
    "    print(accepted_scores)\n",
    "    return training_data\n",
    "\n",
    "\n",
    "def build_model(input_size, output_size):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(128, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(52, activation='relu'))\n",
    "    model.add(Dense(output_size, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(training_data):\n",
    "    X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))\n",
    "    y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))\n",
    "    \n",
    "    model = build_model(input_size=len(X[0]), output_size=len(y[0]))\n",
    "    model.fit(X, y, epochs=10)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate train data and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "episodes = 200\n",
    "score_requirement = -198\n",
    "intial_games = 10000\n",
    "\n",
    "training_data = model_data_preparation(env, intial_games, episodes)\n",
    "trained_model = train_model(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "choices = []\n",
    "for each_game in range(10):\n",
    "    score = 0\n",
    "    memory = []\n",
    "    prev_state = []\n",
    "    for episode in range(episodes):\n",
    "        env.render()\n",
    "        if len(prev_state) == 0:\n",
    "            action = random.randrange(0,2)\n",
    "        else:\n",
    "            action = np.argmax(trained_model.predict(prev_state.reshape(-1, len(prev_obs)))[0])\n",
    "        \n",
    "        choices.append(action)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        memory.append([new_state, action])\n",
    "        \n",
    "        prev_state = new_state\n",
    "        score += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.reset()\n",
    "    scores.append(score)\n",
    "\n",
    "env.close()\n",
    "print(scores)\n",
    "print('Average Score:', sum(scores)/len(scores))\n",
    "print('choice 1:{} choice 0:{} choice 2:{}'.format(choices.count(1) / len(choices),\n",
    "                                                   choices.count(0) / len(choices),\n",
    "                                                   choices.count(2) / len(choices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. DQN solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\szige\\Anaconda3\\envs\\ceu_dl\\lib\\site-packages\\ipykernel_launcher.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A success counter helper class to track results of successful episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Success:\n",
    "\n",
    "    def __init__(self, threshold=10):\n",
    "        self.sum = 0\n",
    "        self.last10 = []\n",
    "        self.last10sum = sum(self.last10)\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def __iadd__(self, value):\n",
    "        self.sum += value\n",
    "        self.last10.append(value)\n",
    "        self.last10 = self.last10[-10:]\n",
    "        self.last10sum = sum(self.last10)\n",
    "        return self\n",
    "\n",
    "    def __add__(self, value):\n",
    "        new = Success()\n",
    "        new.sum = self.sum\n",
    "        new.last10 = self.last10\n",
    "        new.last10sum = self.last10sum\n",
    "        new.threshold = self.threshold\n",
    "        return new.__iadd__(value)\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self.last10) >= self.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.gamma = 0.99  # discount factor\n",
    "\n",
    "        # control exploration vs explitation\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.95\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.num_episodes = 400\n",
    "        self.max_steps = 201  # max is 200\n",
    "        \n",
    "        # NeuralNet parameters\n",
    "        self.learing_rate = 0.001\n",
    "        self.batch_size = 32\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        self.sync_models()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Generate DNN to approximate Q-value.\n",
    "        \n",
    "        Create a network with:\n",
    "        - 1 dense layer with 24 neurons using relu activation\n",
    "        - 1 dense layer with 48 neurons using relu activation\n",
    "        - 1 dense layer with action space neurons using linear activation\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        state_shape = self.env.observation_space.shape\n",
    "        action_shape = self.env.action_space.n\n",
    "\n",
    "        model.add(Dense(24, activation='relu', input_shape=state_shape))\n",
    "        model.add(Dense(48, activation='relu'))\n",
    "        model.add(Dense(action_shape, activation='linear'))\n",
    "        \n",
    "        optimizer = Adam(learning_rate=self.learing_rate)\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    def sync_models(self):\n",
    "        \"\"\"Syncronize the learned and the off-policy model.\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Select next action given the actual state.\"\"\"\n",
    "        # Exploration\n",
    "        if np.random.rand(1) < self.epsilon:\n",
    "            return np.random.randint(0, self.env.action_space.n)\n",
    "        # Exploitation\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Update epsilon value by decaying it until reaching minimum value.\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = max(self.epsilon_min, \n",
    "                               self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        \"\"\"Save episode results for later use.\"\"\"\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def generate_batch(self):\n",
    "        \"\"\"Generate training batch from the saved memory by random sampling.\"\"\"\n",
    "        samples = np.array(random.sample(self.memory, self.batch_size))\n",
    "        \n",
    "        states, actions, rewards, new_states, dones = np.hsplit(samples, 5)\n",
    "\n",
    "        states = np.concatenate(np.squeeze(states[:]), axis=0)    # [batch_size x 2]\n",
    "        new_states = np.concatenate(np.concatenate(new_states))   # [batch_size x 2]\n",
    "        rewards = rewards.reshape(self.batch_size,).astype(float) # [batch_size]\n",
    "        actions = actions.reshape(self.batch_size,).astype(int)   # [batch_size]\n",
    "        dones = dones.reshape(self.batch_size,).astype(bool)      # [batch_size]\n",
    "        notdones = (~dones).astype(float)\n",
    "    \n",
    "        return states, actions, rewards, new_states, notdones\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Updating Q values using the actual model and an off-policy model.\n",
    "        \n",
    "        The update rule is:\n",
    "        Q(s_t, a_t) = Q(s_t, a_t) + alpha * (r_t + gamma * Q^*(s_t+1, a_t) - Q(s_t, a_t))\n",
    "        \n",
    "        Where the model is responsible of generating the Q(s_t, a_t) values, \n",
    "        and the off-policy model is responsible of generating the Q^*(s_t+1, a_t) values.\n",
    "        \n",
    "        The update is performed by the neural network backpropagation by setting the target\n",
    "        value to r_t + gamma * Q^*(s_t+1, a_t).\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # I. Generating examples from memory\n",
    "        states, actions, rewards, new_states, notdones = self.generate_batch()\n",
    "        # II. Generating Q(s_t, a_t)\n",
    "        targets = self.model.predict(states)\n",
    "        indices = np.arange(self.batch_size)\n",
    "        \n",
    "        # III. Using off-policy model to predict future Q values: \n",
    "        # Q^*(s_t+1, a_t)\n",
    "        Q_futures = self.target_model.predict(new_states).max(axis = 1)\n",
    "        \n",
    "        # IV. Generating temporal difference target\n",
    "        # td = r_t + gamma * Q^*(s_t+1, a_t)\n",
    "        targets[(indices, actions)] = rewards + self.gamma * Q_futures * notdones\n",
    "        \n",
    "        # V. Updating Q(s_t, a_t) by executing 1 training step\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "    def optimize_model(self, state, eps, render=True):\n",
    "        score = 0\n",
    "        max_position = -99\n",
    "\n",
    "        for step in range(self.max_steps):\n",
    "            # Show the animation every 50 eps\n",
    "            if render and eps % 50 == 0:\n",
    "                env.render()\n",
    "\n",
    "            # select action\n",
    "            action = self.act(state)\n",
    "            # observe environment after taking action\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            new_state = new_state.reshape(1, 2)\n",
    "\n",
    "            # Keep track of max position\n",
    "            position = new_state[0][0]\n",
    "            if position > max_position:\n",
    "                max_position = position\n",
    "\n",
    "            # Adjust reward for task completion\n",
    "            if position >= 0.5:\n",
    "                reward += 10\n",
    "            \n",
    "            # Save episode results\n",
    "            self.remember(state, action, reward, new_state, done)\n",
    "            # Train network on observed events\n",
    "            self.replay()\n",
    "            \n",
    "            # Update state and value reward\n",
    "            state = new_state\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        self.sync_models()\n",
    "        self.decay_epsilon()\n",
    "        \n",
    "        return step < 199\n",
    "\n",
    "    def fit(self, render=True):\n",
    "        success = Success()\n",
    "        episodes = tqdm(range(self.num_episodes))\n",
    "        for episode in episodes:\n",
    "            state = env.reset().reshape(1, 2)\n",
    "            success += self.optimize_model(state, episode, render)\n",
    "\n",
    "            episodes.set_postfix_str(f'overall: {success.sum}, '\n",
    "                                     f'last10: {success.last10sum}')\n",
    "            if success:\n",
    "                print(f'10 success in a row, stopping early at episode {episode}.')\n",
    "                episodes.close()\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "\n",
    "                \n",
    "def play(env, model, n=1):\n",
    "    for _ in range(n):\n",
    "        done = False\n",
    "        state = env.reset().reshape(1, 2)\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = model.act(state)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            state = new_state.reshape(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263d1879b74142f2ae26450a8e0bc6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 success in a row, stopping early at episode 236.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with Environment('MountainCar-v0') as env:\n",
    "    env.seed(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    dqn = DQN(env=env).fit(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Environment('MountainCar-v0') as env:\n",
    "    play(env, dqn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Solve the [CartPole](https://gym.openai.com/envs/CartPole-v1/) problem\n",
    "\n",
    "Use the DQN algorithm to learn how to balance a pole on top of a moving cart. This is actually an easier task, since our action space is reduced greatYou have to modify the previously created class slightly. Can you make the class more general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good job!\n",
    "\n",
    "Thank you for your attention! Consider solving the exercises from the [DL 10 Home Assignment II.](./DL_10_Home_Assignment_II.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
